{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl"
   },
   "source": [
    "## MODELE SZUMÓW\n",
    "### Biały szum  Levy'ego\n",
    " W literaturze szumem nazywa się dowolny proces stacjonarny. Stacjonarny - oznacza to, że funkcja korelacyjna procesu $\\xi(t)$ zależy tylko od różnicy czasów, tzn. \n",
    "$$C(t, s) = \\langle \\xi(t) \\xi(s) \\rangle = C(t-s)$$\n",
    "Innymi słowy funkcja\n",
    "$$C(t) = \\langle \\xi(s+t) \\xi(s) \\rangle$$\n",
    "nie zależy od czasu $s$, a zależy tylko od czasu $t$.\n",
    "Podaliśmy definicję procesu Levy'ego jako najbardziej ogólnego procesu stochastycznego o niezależnych przyrostach na nieprzekrywających sie przedziałach. Proces ten w szczegolnych przypadkach redukuje się do procesu Wienera i procesu Poissona. Pochodna tego procesu to biały szum Levy'ego  $\\xi(t)$:\n",
    "$$\\xi(t)=\\frac{dL(t)}{dt} \\qquad (13)$$\n",
    "Z  postaci funkcji korelacyjnej dla procesu Levy'ego wynika, że  funkcja korelacyjna białego szumu  Levy'ego  $\\xi(t)$  ma postać\n",
    "$$\\langle \\xi(t) \\xi(s) \\rangle = \\frac{\\partial^2}{\\partial t \\partial  s} \\langle L(t) L(s) \\rangle=    \\frac{\\partial^2}{\\partial t \\partial  s} [2D  \\mbox{min} (t, s)] =2D \\frac{\\partial^2}{\\partial t \\partial  s}  [ t \\theta(s-t) + s \\theta(t-s)] = 2D \\delta (t-s), \\qquad (14)$$ co uzasadnia nazwę \"biały szum\", ponieważ widmo mocy (transformata Fouriera funkcji korelacyjnej) jest niezależna od częstości, podobnie jak dla światła białego.\n",
    "Przy obliczaniu pochodnych należy pamiętać, że $\\theta'(t) = \\delta(t)$ oraz $ t\\delta(t-s) = s \\delta(t-s)$ i $\\delta(t-s) = \\delta(s-t)$.\n",
    "Szczególne przypadki tego szumu to:\n",
    "a:  biały szum gaussowski (pochodna procesu Wienera)\n",
    "b:  biały szum poissonowski, nazywany białym szumem śrutowym lub poissonowskim ciągiem $\\delta$-impulsów  (pochodna procesu Poissona)\n",
    " \n",
    "### Szumy gaussowskie\n",
    "Szumem gaussowskim nazywamy dowolny stacjonarny proces stochastyczny o gaussowskimk rozkładzie prawdopodobieństwa. Jest on najczęściej wykorzystywany w modelowaniu z powodu centralnego twierdzenia granicznego. Szum gaussowski pojawia się przy opisie układu oddziałującego z otoczeniem (termostatem) ponieważ termostat jest układem o wielkiej liczbie stopni swobody (większej od liczby Avogadra    $10^{23}$). Dlatego szum termiczny i równowagowe fluktuacje termiczne modelowane są szumem gaussowskim. Szum taki jest całkowicie określony przed dwie wielkości: wartość średnią oraz funkcję korelacyjną. Z matematycznego punktu widzenia, proces taki możemy opisać tzw. funkcjonałem rozkładu prawdopodobieństwa, który jest uogólnieniem gęstości rozkładu prawdopodobieństwa : \n",
    "$${\\cal D}P[\\xi] = {\\cal D}z \\; {\\mbox{exp}}\\left[-\\frac{1}{2} \\int dt\\int ds\\;\\xi(t) K(t-s) \\xi(s) \\right], \\qquad (16)$$\n",
    "gdzie  ${\\cal D} z$ jest pewną miarą funkcjonalną (podobnie jak iloczyn różniczek i jakobianu transformacji w całkach wielokrotnych) oraz funkcja  $K(t)$   jest odwrotnością funkcji korelacyjnej $C(t)$ w takim sensie, że zachodzi relacja\n",
    "$$\\int ds K(t-s) C(s-u) = \\delta (t-u) \\qquad (17)$$\n",
    "Biały szum gaussowski odpowiada przypadkowi, gdy funkcja korelacyjna ma postać:\n",
    "$$C(t)= 2D_0 \\delta (t), \\qquad (18)$$\n",
    "gdzie  $D_0$ jest intensywnością szumu.  Dla białego szumu gaussowskiego $\\xi(t)$ funkcjonał rozkładu prawdopodobieństwa ma postać:\n",
    "$${\\cal D}P[\\xi] = {\\cal D}z \\; {\\mbox{exp}}\\left[-D_0 \\int dt\\;\\xi^2(t) \\right], \\qquad (20)$$\n",
    "Dlaczego wprowadza sie taki dziwny obiekt matematyczny jak funkcjonał  ${\\cal D}P[\\xi]$. Przypomnijmy sobie, że zmienna losowa o rozkładzie Gaussa jest okreslona przez wartość średnią zmiennej losowej i jej wariancję\n",
    "$$\\sigma^2 = \\langle\\xi^2\\rangle-\\langle\\xi\\rangle^{2}$$\n",
    "Dla białego szumu $ \\langle \\xi^2(t)\\rangle = \\infty$ i dlatego nie istnieje 1-wymiarowy rozkład prawdopodobieństwa $f(x, t)$ ponieważ $\\sigma^2 = \\infty$.  Dlatego należy uogólnić teorię tak, aby opisywała także biały szum gaussowski. To umożliwia  funkcjonał  ${\\cal D}P[\\xi]$.\n",
    "Istnieją także innego typu szumu gaussowskie. Są to szumy skorelowane (kolorowe). Ważnym przykładem jest eksponencjalnie skorelowany szum:\n",
    "$$C(t)= \\frac{D_0}{\\tau_c} \\mbox{exp}\\left( -\\frac{|t|}{\\tau_c}\\right), \\qquad (21)$$\n",
    "where $D_0$ jest intensywnością (natężeniem) szumu oraz $\\tau_c$  nazywa się czasem korelacji szumu. Ten szum jest generowany przez proces Ornsteina-Uhlenbecka, który opiszemy w następnym rozdziale. Kolejnym przykładem szumu skorelowanego jest tzw. szum harmoniczny o funkcji korelacyjnej typu: \n",
    "$$C(t)= a_1 \\mbox{e}^{-a|t|} \\left(\\cos \\omega t + a_2 \\sin \\omega t\\right) \\qquad (22)$$\n",
    "gdzie  $a_1, a_2$  oraz  $a>0$ są stałymi. Ten szum jest opisany równaniem podobnym do równania Newtona dla oscylatora harmonicznego tłumionego.  Ostatnim przykładem jest szum skorelowany algebraicznie, tzn. jego funkcja korelacyjna jest postaci: \n",
    "$$C(t)= C_0 \\left(1+ \\frac{|t|}{\\tau_c}\\right)^{-\\kappa} \\qquad (23)$$\n",
    "gdzie liczba  $\\kappa >0$.\n",
    "Wszystkie trzy przykłady szumu skorelowanego dążą do białego szumu gaussowskiego w pewnych granicznych przypadkach. Aby to pokazać trzeba skorzystać z twierdzenia o ciągach delta-podobnych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## NOISE MODELS\n",
    "### Levy's white noise\n",
    "In the literature, noise is called any stationary process. Stationary - this means that the correlation function of the $\\xi(t)$ process depends only on the time difference, i.e.\n",
    "$$C(t, s) = \\langle \\xi(t) \\xi(s) \\rangle = C(t-s)$$\n",
    "In other words, a function\n",
    "$$C(t) = \\langle \\xi(s+t) \\xi(s) \\rangle$$\n",
    "does not depend on the time of $s$, and depends only on the time $t$.\n",
    "We gave the definition of Levy's process as the most general stochastic process with independent increments on non-overlapping intervals. This process in special cases is reduced to the Wiener process and the Poisson process. The derivative of this process is the white noise of Levy $\\xi(t)$:\n",
    "$$\\xi(t)=\\frac{dL(t)}{dt} \\qquad (13)$$\n",
    "From the form of correlation function for the Levy process, it follows that the correlation function of the white noise of Levy $\\xi(t)$ is\n",
    "$$\\langle \\xi(t) \\xi(s) \\rangle = \\frac{\\partial^2}{\\partial t \\partial  s} \\langle L(t) L(s) \\rangle=    \\frac{\\partial^2}{\\partial t \\partial  s} [2D  \\mbox{min} (t, s)] =2D \\frac{\\partial^2}{\\partial t \\partial  s}  [ t \\theta(s-t) + s \\theta(t-s)] = 2D \\delta (t-s), \\qquad (14)$$ which justifies the name \"white noise\" because the power spectrum (Fourier transform of the correlation function) is independent of the frequency, similar to white light.\n",
    "When calculating derivatives, remember that $\\theta'(t) = \\delta(t)$ and $ t\\delta(t-s) = s \\delta(t-s)$ and $\\delta(t-s) = \\delta(s-t)$.\n",
    "Special cases of this noise are:\n",
    "a: white Gaussian noise (derivative of the Wiener process)\n",
    "b: white Poisson noise, called white shot noise or Poissonian $\\delta$-pulse (derivative of the Poisson process)\n",
    "\n",
    "### Gaussian noise\n",
    "Gaussian noise is defined as any stationary stochastic process with a Gaussian probability distribution. It is most often used in modeling due to the central limit theorem. Gaussian noise appears in the description of the system interacting with the environment (thermostat) because the thermostat is a system with a large number of degrees of freedom (greater than the number Avogadra $10^{23}$). Therefore, thermal noise and equilibrium thermal fluctuations are modeled by Gaussian noise. Such noise is completely specified before two values: mean value and correlation function. From a mathematical point of view, such a process can describe the so-called function of the probability distribution, which is a generalization of the probability distribution density:\n",
    "$${\\cal D}P[\\xi] = {\\cal D}z \\; {\\mbox{exp}}\\left[-\\frac{1}{2} \\int dt\\int ds\\;\\xi(t) K(t-s) \\xi(s) \\right], \\qquad (16)$$\n",
    "where ${\\cal D} z$ is a functional measure (like the product of differentials and Jacobian transformations in multiple integrals) and the $K(t)$ function is the inverse of the $C(t)$ correlation function in the sense that a relationship exists\n",
    "$$\\int ds K(t-s) C(s-u) = \\delta (t-u) \\qquad (17)$$\n",
    "The white Gaussian noise corresponds to the case when the correlation function has the form:\n",
    "$$C(t)= 2D_0 \\delta (t), \\qquad (18)$$\n",
    "where $D_0$ is the intensity of noise. For white Gaussian noise $\\xi(t)$, the probability distribution function has the form:\n",
    "$${\\cal D}P[\\xi] = {\\cal D}z \\; {\\mbox{exp}}\\left[-D_0 \\int dt\\;\\xi^2(t) \\right], \\qquad (20)$$\n",
    "Why is there such a strange mathematical object as ${\\cal D}P[\\xi]$? Recall that a random variable with a Gaussian distribution is determined by the mean value of a random variable and its variance\n",
    "$$\\sigma^2 = \\langle\\xi^2\\rangle-\\langle\\xi\\rangle^{2}$$\n",
    "For white noise $ \\langle \\xi^2(t)\\rangle = \\infty$ and therefore there is no 1-dimensional probability distribution $f(x, t)$ because $\\sigma^2 = \\infty$. Therefore, the theory should be generalized so that it also describes the white Gaussian noise. This is enabled by ${\\cal D}P[\\xi]$.\n",
    "There are also other types of Gaussian noise. They are correlated (colored) noise. An important example is the exponential correlated noise:\n",
    "$$C(t)= \\frac{D_0}{\\tau_c} \\mbox{exp}\\left( -\\frac{|t|}{\\tau_c}\\right), \\qquad (21)$$\n",
    "where $D_0$ is the intensity (intensity) of noise and $\\tau_c$ is sometimes called noise correlation. This noise is generated by the Ornstein-Uhlenbeck trial, which we will describe in the next chapter. Another example of correlated noise is the so-called harmonic noise with a correlation function of the type:\n",
    "$$C(t)= a_1 \\mbox{e}^{-a|t|} \\left(\\cos \\omega t + a_2 \\sin \\omega t\\right) \\qquad (22)$$\n",
    "where $a_1, a_2$ and $a>0$ are constants. This noise is described by an equation similar to the Newton equation for a damped harmonic oscillator. The last example is algebraically correlated noise, i.e. its correlation function is as follows:\n",
    "$$C(t)= C_0 \\left(1+ \\frac{|t|}{\\tau_c}\\right)^{-\\kappa} \\qquad (23)$$\n",
    "where the number $\\kappa >0$.\n",
    "All three examples of correlated noise seek white Gaussian noise in certain extreme cases. To show this you need to use the theorem on delta-like sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alalala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl"
   },
   "source": [
    "### Szumy poissonowskie\n",
    " \n",
    "Poissonowski biały szum  to pochodna procesu Poissona.  Uogólniony proces Poissona można   zapisać przy pomocy funkcji teta Heaviside'a w postaci\n",
    "$$N(t) = \\sum\\limits_i z_i \\theta (t-t_i), \\qquad (24)$$\n",
    "gdzie  $\\theta (x)$ jest funkcją schodkową  Heaviside'a oraz  $\\{t_i\\}$ jest zbiorem losowych chwil skoków o średniej gęstości  $\\mu$.  Amplitudy skoków  $\\{z_i\\}$  sa niezależnymi zmiennymi losowymi o tym  samym rozkładzie prawdopodobieństa  $\\rho(z)$ i są niezależne od $t_i$.   Realizacjami takiego procesu są funkcje schodkowe o skokach w losowych  chwilach czasu $t_i$ i o losowych wielkościach skoku  $z_i$.  Pochodna tego procesu \n",
    "$$\\xi(t) = \\frac{dN(t)}{dt}= \\sum\\limits_i z_i \\delta (t-t_i) \\qquad (26)$$\n",
    "to biały szum poissonowski. Faktycznie jest to losowy ciąg impulsów typu $\\delta$-Diraca.\n",
    "Pamiętajmy, że parameter  $\\mu$  określa średnią liczbę delta-impulsów na jednostkę czasu. Ponieważ średnia wartość procesu Poissona wynosi $\\langle N(t)\\rangle = \\mu\\langle z_i\\rangle t$   więc  średnia wartość białego szumu wynosi\n",
    "$$\\langle \\xi(t) \\rangle = \\frac{d\\langle N(t)\\rangle }{dt} = \\mu \\langle z_i\\rangle $$\n",
    "gdzie\n",
    "$$\\langle z_i \\rangle =  \\int_{-\\infty}^{\\infty}   z \\rho(z)  dz $$\n",
    "Możemy przedefiniowac tak szum, aby jego wartość wynosiła 0. Zdefiniujmy nowy szumy jako\n",
    "$$Y_0(t) = \\sum\\limits_{i} z_i \\delta (t-t_i) -\\mu  \\qquad (27)$$\n",
    "Wartość średnia oraz funkcja korelacyjne tego szumu to:\n",
    "$$\\langle Y_0(t) \\rangle = 0, \\quad \\langle Y_0(t) Y_0(u) \\rangle = 2D_S \\delta (t-u), \\qquad (28)$$\n",
    "gdzie  $D_S=(1/2)\\mu \\langle z_i^2 \\rangle$  nazywa się intensywnością szumu. \n",
    "Symetryczny biały szum Poissona  to taki ciąg impulsów, że gęstość prawdopodobieństwa  $\\rho(z)$ dla rozkładu amplitud jest parzystą (symetryczną)  funkcją. Na przykład rozkład prawdopodobieństwa wielkości skoków\n",
    "$$\\rho(z) = (1/2 A) \\mbox{e}^{- \\vert z \\vert /A}, \\quad A > 0$$\n",
    "jest funkcją symetryczną $z \\to -z$. Dlatego też skoki w górę ( $ z>0$ ) oraz  w dół \n",
    "($z<0$).\n",
    "\n",
    "Jeżeli szum jest asymetryczny, to $\\rho(z)\\ne \\rho(-z)$. Np.\n",
    "$$\\rho(z) = (1/ A^2) z \\mbox{e}^{(- z /A)} \\theta(z) , \\quad A > 0 $$\n",
    "opisuje niesymetryczny biały szum Poissona. Przykładem też  jest rozkład\n",
    "$$\\rho(z) = (1/A) e^{-z/A} \\theta (z), \\quad A>0$$\n",
    "W tym przypadku możliwe są  tylko skoki w górę ($z>0$)  i  ich średnia wartość wynosi:\n",
    "$$\\langle z_i \\rangle = A$$\n",
    "Stąd otrzymujemy interpretację parametru $A$ w powyższym rozkładzie prawdopodobieństwa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Poisson speakers\n",
    "\n",
    "Poisson's white noise is a derivative of the Poisson process. The generalized Poisson process can be written using the Heaviside theta function in the form\n",
    "$$N(t) = \\sum\\limits_i z_i \\theta (t-t_i), \\qquad (24)$$\n",
    "where $\\theta (x)$ is a stepwise function of Heaviside and $\\{t_i\\}$ is a set of random hoppings with an average density of $\\mu$. The mathematical amplitudes of $\\{z_i\\}$ are independent random variables with the same probability $\\rho(z)$ distribution and are independent of $t_i$. The realizations of such a process are stepped functions with jumps at random times of $t_i$ time and with random $z_i$ jumps. A derivative of this process\n",
    "$$\\xi(t) = \\frac{dN(t)}{dt}= \\sum\\limits_i z_i \\delta (t-t_i) \\qquad (26)$$\n",
    "it's white Poisson noise. In fact, this is a random series of $\\delta$-Diraca pulses.\n",
    "Remember that the $\\mu$ parameter is the average number of delta-pulses per unit of time. Since the average value of the Poisson process is $\\langle N(t)\\rangle = \\mu\\langle z_i\\rangle t$, the average value of white noise is\n",
    "$$\\langle \\xi(t) \\rangle = \\frac{d\\langle N(t)\\rangle }{dt} = \\mu \\langle z_i\\rangle $$\n",
    "where\n",
    "$$\\langle z_i \\rangle =  \\int_{-\\infty}^{\\infty}   z \\rho(z)  dz $$\n",
    "We can redefine the noise so that its value is 0. Let's define a new noise as\n",
    "$$Y_0(t) = \\sum\\limits_{i} z_i \\delta (t-t_i) -\\mu  \\qquad (27)$$\n",
    "The average value and correlation function of this noise are:\n",
    "$$\\langle Y_0(t) \\rangle = 0, \\quad \\langle Y_0(t) Y_0(u) \\rangle = 2D_S \\delta (t-u), \\qquad (28)$$\n",
    "where $D_S=(1/2)\\mu \\langle z_i^2 \\rangle$ is called the intensity of noise.\n",
    "The symmetrical white Poisson noise is such a pulse sequence that the probability density $\\rho(z)$ for the amplitude distribution is an even (symmetrical) function. For example, the probability distribution of hop sizes\n",
    "$$\\rho(z) = (1/2 A) \\mbox{e}^{- \\vert z \\vert /A}, \\quad A > 0$$\n",
    "is a symmetrical function $z \\to -z$. That is why jumping up ($ z>0$) and down\n",
    "($z<0$).\n",
    "\n",
    "If the noise is asymmetrical, then $\\rho(z)\\ne \\rho(-z)$. Eg.\n",
    "$$\\rho(z) = (1/ A^2) z \\mbox{e}^{(- z /A)} \\theta(z) , \\quad A > 0 $$\n",
    "describes the unbalanced white noise of Poisson. An example is the distribution\n",
    "$$\\rho(z) = (1/A) e^{-z/A} \\theta (z), \\quad A>0$$\n",
    "In this case, only jumps up ($z>0$) are possible and their average value is:\n",
    "$$\\langle z_i \\rangle = A$$\n",
    "Hence, we get the interpretation of the $A$ parameter in the above probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    reset()\n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "\n",
    "    T = 15\n",
    "    mu = 1.3\n",
    "    N = stats.poisson.rvs(T*mu)\n",
    "\n",
    "    steps = range(N+1)\n",
    "    points = sorted([random()*T for i in steps])\n",
    "    z = stats.expon.rvs(size=N).tolist()\n",
    "    steps2 = np.cumsum(z)\n",
    "\n",
    "    p1 = plot_step_function(zip(points,steps2),figsize=[8,3])\n",
    "    p1.axes_labels([r'$t$',r'$N(t)$'])\n",
    "\n",
    "    p = sum([line(((points[i],0),(points[i],z[i]))) for i in steps[:-1]])\n",
    "    p.axes_labels([r'$t$',r'$\\xi(t)$'])\n",
    "\n",
    "    print \"fig1: Poisson\"\n",
    "    p1.show(figsize=[8,3],frame=1,axes=0)\n",
    "    print \"fig2:  Poissona\"\n",
    "    p.show(figsize=[8,3],frame=1,axes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl"
   },
   "source": [
    "### Szum dychotomiczny (telegraficzny) \n",
    " \n",
    "Podamy teraz przykład niegaussowskiego szumu.  Jest to 2-stanowy proces stochastyczny\n",
    "$$\\xi(t) = \\{-a , b\\}, \\quad a, b > 0. \\qquad (34)$$\n",
    "Szum może przyjmować dwie wartości $-a$ lub $b$. Przeskoki pomiędzy tymi dwoma stanami są dane przez prawdopodobieństwa przejścia  w jednostce czasu\n",
    "$$Pr(-a\\rightarrow b)=\\mu_a = 1/\\tau_a, \\qquad Pr(b\\rightarrow -a)=\\mu_b = 1/\\tau_b, \\qquad (35)$$\n",
    "gdzie  $\\tau _a$ and $\\tau _b$ są średnimi czasami przebywania w stanach  $-a$ oraz  $b$.  Jeżeli założymy że\n",
    "$$b \\mu_a= a \\mu_b \\qquad (36)$$\n",
    "to proces jest stacjonarny o zerowej wartości średniej. Funkcja korelacyjna ma postać funkcji eksponencjalnej (podobnie jak dla szumu Ornsteina-Uhlenbecka) \n",
    "$$C(t) = a b \\:\\mbox{exp}\\left(-\\frac{|t|}{\\tau_c} \\right), \\qquad (37)$$\n",
    "gdzie czas korelacji szumu  $\\tau_c$ dany jest prze formułę \n",
    "$$1/\\tau_c = \\mu_a + \\mu_b$$\n",
    "Szum jest symetryczny gdy $a=b$  i to implikuje że  $\\mu_a=\\mu_b$ gdy $\\langle \\xi(t) \\rangle = 0$. W pewnych przypadkach szum ten może opisywać losowe przejścia pomiędzy dwoma metastabilnymi stanami w układach bistabilnych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Dichotomous (telegraph) noise\n",
    "\n",
    "We will now give an example of non-Gaussian noise. It is a 2-state stochastic process\n",
    "$$\\xi(t) = \\{-a , b\\}, \\quad a, b > 0. \\qquad (34)$$\n",
    "The noise can take two values ​​$-a$ or $b$. The jumps between these two states are given by the probabilities of transition in a unit of time\n",
    "$$Pr(-a\\rightarrow b)=\\mu_a = 1/\\tau_a, \\qquad Pr(b\\rightarrow -a)=\\mu_b = 1/\\tau_b, \\qquad (35)$$\n",
    "where $\\tau _a$ and $\\tau _b$ are average times in the $-a$ and $b$ states. If we assume that\n",
    "$$b \\mu_a= a \\mu_b \\qquad (36)$$\n",
    "this process is stationary with a zero average value. The correlation function takes the form of an exponential function (similar to the Ornstein-Uhlenbeck noise)\n",
    "$$C(t) = a b \\:\\mbox{exp}\\left(-\\frac{|t|}{\\tau_c} \\right), \\qquad (37)$$\n",
    "where the $\\tau_c$ noise correlation time is given to the formula\n",
    "$$1/\\tau_c = \\mu_a + \\mu_b$$\n",
    "The noise is symmetrical when $a=b$ and it implies that $\\mu_a=\\mu_b$ when $\\langle \\xi(t) \\rangle = 0$. In some cases this noise can describe random transitions between two metastable states in bistable systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    reset()\n",
    "    from scipy import stats\n",
    "    from numpy import cumsum\n",
    "\n",
    "    # definicja szumu dychotomicznego\n",
    "    a = -1\n",
    "    b = 3\n",
    "    stan = [a,b]\n",
    "    mu_a = 1\n",
    "    mu_b = mu_a * abs(b) / abs(a)\n",
    "    mu = [mu_a,mu_b]\n",
    "\n",
    "    # realizacja\n",
    "    N = 20\n",
    "    czasy = [-log(random()/mu[i%2]) for i in range(N)]\n",
    "    punkty = cumsum(czasy)\n",
    "    stany = [stan[i%2] for i in range(N)]\n",
    "\n",
    "    # wizualizacja\n",
    "    p = plot_step_function(zip(punkty,stany))\n",
    "    p.axes_labels([r'$t$',r'$\\xi(t)$'])\n",
    "    p.show(figsize=[8,3],frame=1,axes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl"
   },
   "source": [
    "### Szum kangura \n",
    " \n",
    "Obserwacje kangurów prowadzą do ciekawych uogólnień w teorii procesów stochastycznych. Kangur od czasu do czasu skacze. Skoki kangura są o różnej długości.  Pomiędzy skokami kangur spoczywa przez jakiś czas. To zachowanie  kangura prowadzi do następującego opisu procesu losowego kangura $\\xi(t)$:  Jest to proces nieciągły i  stacjonarny w którym prawdopodobieństwo przejścia ${\\cal W}(z \\vert z_0)$  ze stanu $z_0$ do stanu $z$ faktoryzuje się, tzn. \n",
    "$${\\cal W}(z \\vert z_0) = Q(z) \\nu (z_0) $$\n",
    "To oznacza, że kangur skacze ze stanu $z_0$ ze średnią  częstością  $\\nu(z_0)$. Wielkość $\\tau(z_0) = 1/\\nu(z_0)$ to średni czas przebywania w stanie $z_0$. Prawdopodobieństwo, że kangur skoczy do stanu  $z$  wynosi  $Q(z)$ i jest unormowane do 1 w przestrzeni stanów kangura.  Odpowiednie równanie ewolucji dla gęstości prawdopodobieństwa   $p(z, t)$ dla procesu kangura ma postać:\n",
    "$${\\frac{\\partial  p(z, t)}{\\partial t}} = - \\nu (z) p(z, t) + Q(z) \\int_{-\\infty}^{\\infty} \\nu (\\eta) p(\\eta, t) d\\eta  $$\n",
    "W przypadku stanów stacjonarnych, gdy $p(z, t) = p(z)$, otrzymujemy z powyższego równania związek pomiędzy rozkładem stacjonarnym $p(z)$ oraz gęstością prawdopodobieństwa $Q(z)$:\n",
    "$$Q(z) = \\frac{\\nu (z) p(z)}{ \\int_{-\\infty}^{\\infty} \\nu (\\eta) p(\\eta, t) d\\eta } =  \\frac{\\nu (z) p(z)}{\\langle \\nu \\rangle} \\qquad (1) $$\n",
    "Dla symetrycznego procesu kangura funkcja korelacyjna jest  dana przez  relację (nie jest łatwym zadaniem to pokazać)\n",
    "$$C(t) = 2 \\int_{0}^{\\infty} z^2 p(z) \\mbox{exp}(-\\nu(z)\\vert t\\vert) \\;dz, $$\n",
    "gdzie  $p(z) = p(-z)$  jest rozkładem stacjonarnym procesu  $\\xi(t) $  oraz  $\\nu(z) = \\nu(-z)$.  W tym przypadku watość średnia procesu jest zero, $\\langle \\xi(t) \\rangle = 0$.\n",
    "Specjalnym przypadkiem tego procesu jest tzw. szum Kubo-Andersona gdy  $\\nu(z) = \\nu_0$.  Z rów. (1) wynika, że dla stałej częstości przeskoków stacjonarna gęstość $p(z) = Q(z)$.   Wówczas z powyższej relacji otrzymujemy jawną postać funkcji korelacyjnej:\n",
    "$$C(t) = \\langle z^2 \\rangle \\mbox{exp}\\left(-\\frac{\\vert t\\vert}{\\tau_c}\\right) $$\n",
    "gdzie czas korelacji szumu wynosi  $\\tau_c = 1/\\nu_0$  oraz  $\\langle z^2 \\rangle$ jest wartością średnią  obliczoną z rozkładem  stacjonarnym $p(z) = Q(z)$. \n",
    "Oto dwa przyklady szumów kangura:\n",
    "(i) proces $\\xi(t) $  jest nieograniczony, określony na przedziale  $(-\\infty, \\infty)$ i stacjonarna gęstość prawdopodobieństwa jest gaussowska:\n",
    "$$p(z) = Q(z) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\mbox{exp}(-z^2/2\\sigma^2), \\quad \\xi(t) \\in (-\\infty, \\infty)$$\n",
    "(ii) proces $\\xi(t) $  jest ograniczony na przedziale  $[-l, l]$ i ma jednostajną gęstość prawdopodobieństwa\n",
    "$$p(z) = Q(z) = \\frac{1}{2l}\\theta(z+l)\\theta(l-z),\\quad \\xi(t) \\in [-l, l], \\qquad (43)$$\n",
    "gdzie  $\\theta(x)$  jest funkcją schodkową Heaviside'a. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Noise of the kangaroo\n",
    "\n",
    "Observations of kangaroos lead to interesting generalizations in the theory of stochastic processes. Kangaroo jumps from time to time. The kangaroo jumps are of different lengths. Between kangaroo leaps rests for some time. This behavior of the kangaroo leads to the following description of the random kangaroo process $\\xi(t)$: It is a discontinuous and stationary process in which the probability of passing ${\\cal W}(z \\vert z_0)$ from the $z_0$ state to the $z$ state becomes factored, i.e.\n",
    "$${\\cal W}(z \\vert z_0) = Q(z) \\nu (z_0) $$\n",
    "This means that the kangaroo jumps from the $z_0$ state with the average $\\nu(z_0)$ frequency. The size of $\\tau(z_0) = 1/\\nu(z_0)$ is the average residence time in the $z_0$ state. The probability that the kangaroo will jump to the $z$ state is $Q(z)$ and is normalized to 1 in the kangaroo state space. The appropriate evolution equation for the probability density $p(z, t)$ for the kangaroo process is:\n",
    "$${\\frac{\\partial  p(z, t)}{\\partial t}} = - \\nu (z) p(z, t) + Q(z) \\int_{-\\infty}^{\\infty} \\nu (\\eta) p(\\eta, t) d\\eta  $$\n",
    "In the case of stationary conditions, when $p(z, t) = p(z)$, we obtain from the above equation the relation between the $p(z)$ stationary distribution and the probability density $Q(z)$:\n",
    "$$Q(z) = \\frac{\\nu (z) p(z)}{ \\int_{-\\infty}^{\\infty} \\nu (\\eta) p(\\eta, t) d\\eta } =  \\frac{\\nu (z) p(z)}{\\langle \\nu \\rangle} \\qquad (1) $$\n",
    "For a symmetric kangaroo process, the correlation function is given by the relationship (it is not an easy task to show it)\n",
    "$$C(t) = 2 \\int_{0}^{\\infty} z^2 p(z) \\mbox{exp}(-\\nu(z)\\vert t\\vert) \\;dz, $$\n",
    "where $p(z) = p(-z)$ is the stationary distribution of the $\\xi(t) $ and $\\nu(z) = \\nu(-z)$ process. In this case, the average process value is zero, $\\langle \\xi(t) \\rangle = 0$.\n",
    "A special case of this process is the so-called Kubo-Anderson noise when $\\nu(z) = \\nu_0$. From the equation (1) shows that for a constant hop frequency stationary density $p(z) = Q(z)$. Then, from the above relation, we obtain an explicit form of the correlation function:\n",
    "$$C(t) = \\langle z^2 \\rangle \\mbox{exp}\\left(-\\frac{\\vert t\\vert}{\\tau_c}\\right) $$\n",
    "where the noise correlation time is $\\tau_c = 1/\\nu_0$ and $\\langle z^2 \\rangle$ is the mean value calculated with the $p(z) = Q(z)$ stationary distribution.\n",
    "Here are two examples of kangaroo noise:\n",
    "(i) the $\\xi(t) $ process is unlimited, determined on the $(-\\infty, \\infty)$ interval and the stationary probability density is Gaussian:\n",
    "$$p(z) = Q(z) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\mbox{exp}(-z^2/2\\sigma^2), \\quad \\xi(t) \\in (-\\infty, \\infty)$$\n",
    "(ii) the $\\xi(t) $ process is limited on the $[-l, l]$ interval and has a uniform probability density\n",
    "$$p(z) = Q(z) = \\frac{1}{2l}\\theta(z+l)\\theta(l-z),\\quad \\xi(t) \\in [-l, l], \\qquad (43)$$\n",
    "where $\\theta(x)$ is a step function of Heaviside."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #szum kangura\n",
    "    #szum Kubo - Andersona\n",
    "    #stała częstość \\nu_0\n",
    "    #proces ograniczony\n",
    "\n",
    "    reset()\n",
    "    from scipy import stats\n",
    "\n",
    "    l = 2\n",
    "    N = 20\n",
    "    ksi = [2*l*random() - l for i in range(N)]\n",
    "    #list_plot(ksi)\n",
    "\n",
    "    nu_0 = 2.2\n",
    "    czasy = stats.expon.rvs(scale=1/nu_0,size=N)\n",
    "\n",
    "    p = plot_step_function(zip(czasy,ksi))\n",
    "    p.axes_labels([r'$t$',r'$\\xi$'])\n",
    "    p.show(figsize=[8,3], frame=1, axes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl"
   },
   "source": [
    "### Uwagi ogólne: proces Markowa i proces niemarkowowski\n",
    "Rozważmy dla prostoty równanie stochastyczne w postaci:\n",
    "$$\\dot x = F(x) + G(x) \\xi(t)$$\n",
    "gdzie $\\xi(t)$ jest dowolnym opisanym powyżej szumem. Co możemy powiedzieć o procesie stochastycznym $x=x(t)$ gererowanym przez powyższe równanie. Jeżeli szum  $\\xi(t)$   is  białym szumem wówczas proces  x(t) jest procesem Markowa.  To jest ogromna zaleta ponieważ proces Markowa jest całkowicie opisany gdy znamy \n",
    "(i) warunkową gęstość prawdopodobieństwa $p(x_2, t_2|x_1, t_1)$\n",
    "(ii) stan początkowy procesu $x(t)$ dany przez rozkład 1-wymiarowy  $p(x, 0)$.    Wówczas spełnione jest równanie  Chapmana-Kołmogorowa z którego można wyprowadzić równanie Fokkera-Plancka lub  Kolmogorowa-Fellera. Rozkłady wielowymiarowe wyrażają się przez powyższe 2 funkcje.  \n",
    "Jeżeli szum nie jest szumem białym (jest np. eksponencjalnie skorelowanym) to proces $x(t)$ jest procesem niemarkowowskim. Rozkłady wielowymiarowe nie można otrzymać z powyższych 2 funkcji. Wówczas wiemy  niewiele o takim procesie i jego analiza bywa bardzo złożona."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### General remarks: Markov process and the Niemarkovsky process\n",
    "Consider for simplicity the stochastic equation in the form:\n",
    "$$\\dot x = F(x) + G(x) \\xi(t)$$\n",
    "where $\\xi(t)$ is any noise described above. What can we say about the stochastic $x=x(t)$ process guided by the above equation. If $\\xi(t)$ noise and white noise then the x (t) process is a Markov process. This is a huge advantage because the Markov process is completely described as we know it\n",
    "(i) conditional probability density $p(x_2, t_2|x_1, t_1)$\n",
    "(ii) the initial state of the $x(t)$ process given by the 1-dimensional $p(x, 0)$ distribution. Then the Chapman-Kolmogorov equation is fulfilled, from which the Fokker-Planck or Kolmogorov-Feller equation can be derived. Multidimensional distributions are expressed through the above two functions.\n",
    "If the noise is not white noise (it is, for example, exponentially correlated), the $x(t)$ process is a non-banking process. Multidimensional distributions can not be obtained from the above two functions. Then we know little about such a process and its analysis can be very complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 8.7",
   "language": "",
   "name": "sagemath"
  },
  "language": "python",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "nbTranslate": {
   "displayLangs": [
    "pl",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "pl",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
